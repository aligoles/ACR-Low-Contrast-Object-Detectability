{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef668c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pydicom\n",
    "from pydicom import dcmread\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "from math import pi\n",
    "from scipy.ndimage import label\n",
    "from scipy import signal\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "import glob\n",
    "import statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10abb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reverse(tuples):\n",
    "    new_tup = tuples[::-1]\n",
    "    return new_tup\n",
    "\n",
    "def sort_dicom_files(list_of_files):\n",
    "    \"\"\"\n",
    "    This function sorts DICOM files in a directory based on their Slice Location.\n",
    "\n",
    "    Args:\n",
    "        list_of_files (list): The list of DICOM files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sorted DICOM file.\n",
    "    \"\"\"\n",
    "    # Try to extract the Slice Location header from the first file\n",
    "    try:\n",
    "        dcmread(list_of_files[0]).SliceLocation \n",
    "        slice_order_exists = True\n",
    "    except AttributeError:\n",
    "        slice_order_exists = False\n",
    "\n",
    "    if slice_order_exists:\n",
    "        # Go through each DICOM file and extract the Slice Location header\n",
    "        slice_order = [float(dcmread(f).SliceLocation) for f in list_of_files]\n",
    "\n",
    "        # Use zip to combine the lists, then sort by the slice order\n",
    "        combined = sorted(zip(slice_order, list_of_files))\n",
    "\n",
    "        # The sorted function returns a list of tuples, so you can use zip again to separate them back into two lists\n",
    "        _, sorted_files = zip(*combined)\n",
    "    else:\n",
    "        # If SliceLocation does not exist, sort the files based on their filenames\n",
    "        sorted_files = sorted(list_of_files)\n",
    "    if dcmread(list_of_files[0]).Manufacturer == \"Philips\":\n",
    "        sorted_files  = Reverse(sorted_files)\n",
    "\n",
    "    return list(sorted_files)\n",
    "\n",
    "\n",
    "\n",
    "def read_dicom_series(directory):\n",
    "    \"\"\"Reads a DICOM Series files in the given directory.\n",
    "    Only filesnames matching filepattern will be considered\"\"\"\n",
    "    print(directory)\n",
    "    # lstFilesDCM = natsort.natsorted(glob.glob(os.path.join(directory, filepattern)))\n",
    "    # removing dependancy on natsort. Will ask Ali later why he used it.\n",
    "    lstFilesDCM = glob.glob(directory + \"*.dcm\")\n",
    "    if not lstFilesDCM:\n",
    "        lstFilesDCM = glob.glob(directory + \"*.ima\") #check for dicom files instead\n",
    "    if not lstFilesDCM:\n",
    "        lstFilesDCM = glob.glob(directory+\"*\")\n",
    "        \n",
    "    if not lstFilesDCM: #empty list = no files found in folder/zip file\n",
    "        raise RuntimeError(\"Files not found! Check that there are either .IMA or .dcm files within the zip file.\")\n",
    "    # sort the files\n",
    "    lstFilesDCM = sort_dicom_files(lstFilesDCM)\n",
    "\n",
    "    # Get ref file\n",
    "    RefDs = dcmread(lstFilesDCM[0])\n",
    "    # Load dimensions based on the number of rows, columns, and slices (along the Z axis)\n",
    "    ConstPixelDims = (int(RefDs.Rows), int(RefDs.Columns), len(lstFilesDCM))\n",
    "\n",
    "    # The array is sized based on 'ConstPixelDims'\n",
    "    ArrayDicom = np.zeros(ConstPixelDims, dtype=RefDs.pixel_array.dtype)\n",
    "\n",
    "    # loop through all the DICOM files\n",
    "    for filenameDCM in lstFilesDCM:\n",
    "        # read the file\n",
    "        ds = dcmread(filenameDCM)\n",
    "        # store the raw image data\n",
    "        ArrayDicom[:, :, lstFilesDCM.index(filenameDCM)] = ds.pixel_array\n",
    "\n",
    "    pixel_size = ds.PixelSpacing[0]\n",
    "    phase_encoding = ds.InPlanePhaseEncodingDirection\n",
    "    scan_date = ds.ContentDate\n",
    "    return ArrayDicom, pixel_size, phase_encoding, scan_date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bcfe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def angle_profile_radial(image, image_profile, cX, cY):\n",
    "\n",
    "    # Get indices where the profile image is not zero\n",
    "    coords = np.argwhere(image_profile == 1)\n",
    "\n",
    "    # Calculate the profile, distances, and store results\n",
    "    X, Y = coords[:, 0], coords[:, 1]\n",
    "    profile = image[X, Y]\n",
    "    distances = np.sqrt((cX - X) ** 2 + (cY - Y) ** 2)\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'profile': profile,\n",
    "        'X': X,\n",
    "        'Y': Y,\n",
    "        'distance': distances\n",
    "    })\n",
    "\n",
    "    # Sort the DataFrame by distance\n",
    "    df.sort_values(by='distance', inplace=True, ascending=True, ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff401922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_profile_separate(spoke_number):\n",
    "    '''\n",
    "    This function generates expected reference profile depending on the spoke number\n",
    "    '''\n",
    "    profile = np.zeros((90,3))\n",
    "    pixel_size = 0.5\n",
    "    radi = [12/0.5, 25/0.5, 38/0.5] ## in milimeter\n",
    "    D = [7, 6.39, 5.78, 5.17, 4.55, 3.94, 3.33, 2.72, 2.11, 1.5]\n",
    "    d = D[spoke_number-1]\n",
    "    for i,r in enumerate(radi):\n",
    "        profile[round(r-d):round(r+d),i] = 1\n",
    "    return profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ffcbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_profile(p,M):\n",
    "    ## This function gets a 1D array of profile (p) and a background threshold (M)\n",
    "    ## and eliminate backgrounds from the begining and end of the profile\n",
    "    ## and returns the index of the first and last element with higher value than M\n",
    "\n",
    "    indices = np.where(p >= M)[0]\n",
    "    \n",
    "    # Return the first and last indices\n",
    "    if indices.size == 0:\n",
    "        return None, None  # Handle edge case where no value meets the condition\n",
    "    idx_l, idx_h = indices[0], indices[-1]\n",
    "    \n",
    "    return idx_l, idx_h\n",
    "\n",
    "def angle_image(alpha_degree,cX,cY,X,Y):\n",
    "    '''\n",
    "    This function gets an angle and generate a 1D profile of the image values along the angle\n",
    "    '''\n",
    "    Im = np.zeros((X, Y))\n",
    "    alpha = np.radians(alpha_degree)\n",
    "\n",
    "    if alpha_degree == 90:\n",
    "        Im[cX, cY:Y] = 1  # Direct assignment for vertical line\n",
    "    elif 45 <= abs(alpha_degree) < 135:\n",
    "        y_range = np.arange(cY, Y)\n",
    "        x_values = cX + (y_range - cY) / np.tan(alpha)\n",
    "        valid_indices = (x_values >= 0) & (x_values < X-1)\n",
    "        Im[np.round(x_values[valid_indices]).astype(int), y_range[valid_indices]] = 1\n",
    "    elif 135 <= abs(alpha_degree) < 225:\n",
    "        x_range = np.arange(cX, X)\n",
    "        y_values = cY + np.tan(alpha) * (x_range - cX)\n",
    "        valid_indices = (y_values >= 0) & (y_values < Y-1)\n",
    "        Im[x_range[valid_indices], np.round(y_values[valid_indices]).astype(int)] = 1\n",
    "    elif 225 <= abs(alpha_degree) < 315:\n",
    "        y_range = np.arange(0, cY)\n",
    "        x_values = cX + (y_range - cY) / np.tan(alpha)\n",
    "        valid_indices = (x_values >= 0) & (x_values < X-1)\n",
    "        Im[np.round(x_values[valid_indices]).astype(int), y_range[valid_indices]] = 1\n",
    "    else:\n",
    "        x_range = np.arange(0, cX)\n",
    "        y_values = cY + np.tan(alpha) * (x_range - cX)\n",
    "        valid_indices = (y_values >= 0) & (y_values < Y-1)\n",
    "        Im[x_range[valid_indices], np.round(y_values[valid_indices]).astype(int)] = 1\n",
    "    \n",
    "    return Im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa35dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_for_max_corr(x,y,max_shift):\n",
    "    '''\n",
    "    This function slightly jitter the generated 1D profile to align with the reference profile of a specific spoke\n",
    "    '''\n",
    "    cc_vec = []\n",
    "    shifts = np.arange(-max_shift, max_shift + 1)\n",
    "    \n",
    "    for shift in shifts:\n",
    "        y_shifted = np.roll(y, shift)\n",
    "        # Handle boundary conditions\n",
    "        if shift < 0:\n",
    "            y_shifted[shift:] = y_shifted[shift - 1]\n",
    "        elif shift > 0:\n",
    "            y_shifted[:shift] = y_shifted[shift - 1]\n",
    "        \n",
    "        # Compute correlation\n",
    "        cc_vec.append(np.corrcoef(x, y_shifted)[0, 1])\n",
    "\n",
    "    # Find the optimal shift with the maximum correlation\n",
    "    optimal_shift_idx = np.argmax(cc_vec)\n",
    "    optimal_shift = shifts[optimal_shift_idx]\n",
    "    y_aligned = np.roll(y, optimal_shift)\n",
    "\n",
    "    return x, y_aligned, optimal_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ffe5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_stat_test(image_thresholded,angle,spoke_number,cX,cY):\n",
    "    '''\n",
    "    This function performes statistical tests for s specific angle\n",
    "    '''\n",
    "\n",
    "    # Normalize the image\n",
    "    image_min, image_max = np.min(image_thresholded), np.max(image_thresholded)\n",
    "    image_normalized = (image_thresholded - image_min) / (image_max - image_min)\n",
    "    [X, Y] = image_thresholded.shape\n",
    "\n",
    "    # Generate angle image and radial profile\n",
    "    Im = angle_image(angle, cX, cY,X,Y)\n",
    "    df = angle_profile_radial(image_normalized, Im, cX, cY)\n",
    "    p = np.array(df['profile'])\n",
    "\n",
    "    # Truncate the profile\n",
    "    idx_l, idx_h = truncate_profile(p, 0.5)\n",
    "    if idx_l is None or idx_h is None or idx_l >= idx_h:\n",
    "        return None, None  # Handle invalid profiles gracefully\n",
    "\n",
    "    # Process the truncated profile\n",
    "    part = p[idx_l:idx_h]\n",
    "    part_resampled = signal.resample(part, 90)\n",
    "    part_short = part_resampled[:-5]\n",
    "\n",
    "    # Polynomial detrending\n",
    "    num = len(part_short)\n",
    "    x_short = np.linspace(0, num, num)\n",
    "    model = np.polyfit(x_short, part_short, 2)\n",
    "    predicted = np.polyval(model, np.linspace(0, len(part_resampled), len(part_resampled)))\n",
    "    part_detrended = part_resampled - predicted\n",
    "\n",
    "    # Thresholding\n",
    "    mean_part = np.mean(part_detrended)\n",
    "    std_part = np.std(part_detrended)\n",
    "    l_thr, h_thr = mean_part - 2 * std_part, mean_part + 2 * std_part\n",
    "    part_detrended[-5:] = np.where(\n",
    "        (part_detrended[-5:] < l_thr) | (part_detrended[-5:] > h_thr), 0, part_detrended[-5:]\n",
    "    )\n",
    "\n",
    "    # Smoothing\n",
    "    kernel = np.ones(3) / 3  # Kernel size 3\n",
    "    part_smoothed = np.convolve(part_detrended - mean_part, kernel, mode='same')\n",
    "\n",
    "    # Cross-correlation for alignment\n",
    "    profile_all = np.sum(template_profile_separate(spoke_number), axis=1)\n",
    "    _, part_smoothed, _ = shift_for_max_corr(profile_all, part_smoothed, 5)\n",
    "\n",
    "    # Prepare GLM\n",
    "    y = part_smoothed.reshape(90, 1)\n",
    "    predicted = predicted.reshape(90, 1)\n",
    "    profiles_with_bias = np.append(template_profile_separate(spoke_number), np.ones((90, 1)), axis=1)\n",
    "\n",
    "    # Fit the GLM\n",
    "    glm_model = sm.GLM(y, profiles_with_bias)\n",
    "    glm_results = glm_model.fit()\n",
    "\n",
    "    # Return statistical results\n",
    "    return glm_results.pvalues, glm_results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ACR_LCD(BIN_FILE):\n",
    "    '''\n",
    "    This function recieves the directory where the dicom images are store and generates results for the\n",
    "    low contrast detectability test\n",
    "    '''\n",
    "    cmap = \"Greys_r\"\n",
    "    to_analyze = [10, 9, 8, 7]  # slices to analyze\n",
    "    output_list = [['pass']*10]*4\n",
    "    images_all, pixel_size, phase_encoding, scan_date = read_dicom_series(BIN_FILE) # Reading dicom files\n",
    "\n",
    "    [X, Y, N] = images_all.shape\n",
    "\n",
    "    angles = [-23, -16, -9, -2] # initial angles of the first spoke for each slice\n",
    "\n",
    "    # Precompute threshold structure\n",
    "    structure = np.ones((3, 3), dtype=np.int)\n",
    "    for ind, slice_num in enumerate(to_analyze):\n",
    "        image = images_all[:, :, slice_num]\n",
    "\n",
    "        # Normalize the image\n",
    "        image_normalized = image / np.max(image)\n",
    "\n",
    "        #  thresholding to extract central disk\n",
    "        for thr in np.arange(0.05, 0.65, 0.001):\n",
    "            ret, thresh = cv2.threshold(image_normalized, thr, 1, 0)\n",
    "            labeled, ncomponents = label(thresh, structure)\n",
    "            thresh_inner = labeled == np.max(labeled)\n",
    "            if np.sum(thresh_inner != 0) / np.sum(image_normalized != 0) > 0.1 and np.sum(thresh_inner != 0) / np.sum(image_normalized != 0) < 0.2:\n",
    "                break\n",
    "\n",
    "        # Fill the holes and apply threshold\n",
    "        thresh_filled = ndimage.binary_fill_holes(thresh, structure=np.ones((5,5))).astype(int)\n",
    "        image_thresholded = image * thresh_inner\n",
    "        # find center of the disk\n",
    "        M_inner = cv2.moments(np.uint8(thresh_inner))\n",
    "        cY_inner = int(M_inner[\"m10\"] / M_inner[\"m00\"])\n",
    "        cX_inner = int(M_inner[\"m01\"] / M_inner[\"m00\"])\n",
    "\n",
    "        Im_all = np.zeros((X, Y))\n",
    "        pass_vec = np.zeros(10)\n",
    "        p_vals_vec = []\n",
    "        params_vec = []\n",
    "        angle_vec = []\n",
    "        spoke_vec = []\n",
    "        the_angle = angles[ind]\n",
    "\n",
    "        for spoke_number, alpha_degree_initial in enumerate(range(the_angle, -360, -36)):\n",
    "            for angle in np.arange(alpha_degree_initial - 8, alpha_degree_initial + 8, 1 / (1 + spoke_number)):\n",
    "                p_vals, params = angle_stat_test(image_thresholded, angle, spoke_number + 1, cX_inner, cY_inner)\n",
    "                params_vec.extend(params[0:3])\n",
    "                p_vals_vec.extend(p_vals[0:3])\n",
    "                spoke_vec.append(spoke_number)\n",
    "                angle_vec.append(angle)\n",
    "\n",
    "        # pefrom statistical analysis with FDR correction on all spokes of a slice\n",
    "        p_vals_vec_fdr = statsmodels.stats.multitest.fdrcorrection(p_vals_vec, alpha=0.0125, method='indep', is_sorted=False)\n",
    "        pvals_fdr = p_vals_vec_fdr[0].reshape(-1, 3)\n",
    "        params_vec_fdr = np.array(params_vec).reshape(-1, 3)\n",
    "\n",
    "        # check if all three p-values of a spoke pass significant threshold\n",
    "        for i, g in enumerate(pvals_fdr):\n",
    "            if np.sum(g) == 3 and np.sum(params_vec_fdr[i, :] > 0) == 3:\n",
    "                Im = angle_image(angle_vec[i], cX_inner, cY_inner,X,Y)\n",
    "                Im_all = np.logical_or(Im_all, Im)\n",
    "                pass_vec[spoke_vec[i]] += 1\n",
    "\n",
    "        # Create output image\n",
    "        pass1 = np.sum(pass_vec != 0)\n",
    "        image_normalized = (image_thresholded - np.min(image_thresholded)) / (np.max(image_thresholded) - np.min(image_thresholded))\n",
    "        Im_combine = 0.1 * Im_all + image_normalized\n",
    "        plt.figure(figsize = (7,7))\n",
    "        plt.title(\"Slice: \"+str(slice_num+1))\n",
    "        plt.imshow(Im_combine, cmap = cmap)\n",
    "        plt.show()\n",
    "\n",
    "        strs = np.array([\"Pass\" for _ in range(len(pass_vec))])\n",
    "        strs[pass_vec==0] = 'Fail'\n",
    "        output_list[ind] = list(strs)\n",
    "\n",
    "    df_index = ['Slice 11', 'Slice 10', 'Slice 9', 'Slice 8']\n",
    "\n",
    "    df_output = pd.DataFrame(output_list,columns=['Spoke1','Spoke2','Spoke3','Spoke4','Spoke5','Spoke6','Spoke7','Spoke8','Spoke9'\n",
    "                                  ,'Spoke10'],index = df_index)\n",
    "\n",
    "    return df_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5915bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BIN_FILE = input(\"Directory of the ACR DICOM files: \")\n",
    "#BIN_FILE = \"L:/AG_MRI/ACR_lowcontrast_study/Datasets/Dataset01/\"\n",
    "output = ACR_LCD(BIN_FILE)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839283b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
